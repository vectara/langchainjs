---
hide_table_of_contents: true
---

# Output Parser Types

This is a list of the most popular output parsers LangChain supports. The table below has various pieces of information:

**Name**: The name of the output parser

**Supports Streaming**: Whether the output parser supports streaming.

**Has Format Instructions**: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.

**Calls LLM**: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.

**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.

**Output Type**: The output type of the object returned by the parser.

**Description**: Our commentary on this output parser and when to use it.

| Name                                | Supports Streaming | Has Format Instructions       | Calls LLM | Input Type                       | Output Type              | Description                                                                                                                                                                                                                      |
| ----------------------------------- | ------------------ | ----------------------------- | --------- | -------------------------------- | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [String](./string)                  | ✅                 |                               |           | `string` or `Message`            | `string`                 | Takes language model output (either an entire response or as a stream) and converts it into a string. This is useful for standardizing chat model and LLM output and makes working with chat model outputs much more convenient. |
| [HTTPResponse](./http_response)     | ✅                 |                               |           | `string` or `Message`            | `binary`                 | Allows you to stream LLM output properly formatted bytes a web [HTTP response](https://developer.mozilla.org/en-US/docs/Web/API/Response) for a variety of content types.                                                        |
| [OpenAIFunctions](./json_functions) | ✅                 | (Passes `functions` to model) |           | `Message` (with `function_call`) | JSON object              | Allows you to use OpenAI function calling to structure the return output. If you are using a model that supports function calling, this is generally the most reliable method.                                                   |
| [CSV](./csv)                        |                    | ✅                            |           | `string` or `Message`            | `string[]`               | Returns a list of comma separated values.                                                                                                                                                                                        |
| [OutputFixing](./output_fixing)     |                    |                               | ✅        | `string` or `Message`            |                          | Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.                                                                      |
| [Structured](./structured)          |                    | ✅                            |           | `string` or `Message`            | `Record<string, string>` | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.                    |
